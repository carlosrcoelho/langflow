{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u26d3\ufe0f LangFlow","text":"<p>     ~ A User Interface For     LangChain \ud83e\udd9c\ud83d\udd17 ~ </p> <p></p> <p> </p> <p> </p> <p></p> <p>LangFlow is a GUI for LangChain, designed with react-flow.  The drag-and-drop feature provides a quick and effortless way to experiment and prototype, and the built-in chat interface enables real-time interaction. With LangFlow, you can edit prompt parameters, create chains and agents, track the agent\u2019s thought process, and export your flow.</p>"},{"location":"GCP_DEPLOYMENT/","title":"Deploy Langflow on Google Cloud Platform","text":"<p>Follow our step-by-step guide to deploy Langflow on Google Cloud Platform (GCP) using Google Cloud Shell. The guide is available in the Langflow in Google Cloud Platform document.</p> <p></p> <p>Alternatively, click the \"Open in Cloud Shell\" button below to launch Google Cloud Shell, clone the Langflow repository, and start an interactive tutorial that will guide you through the process of setting up the necessary resources and deploying Langflow on your GCP project.</p> <p></p>"},{"location":"GCP_DEPLOYMENT/#run-langflow-from-a-new-google-cloud-project","title":"Run Langflow from a New Google Cloud Project","text":"<p>This guide will help you set up a Langflow development VM in a Google Cloud Platform project using Google Cloud Shell.</p> <p>Note: When Cloud Shell opens, be sure to select Trust repo. Some <code>gcloud</code> commands might not run in an ephemeral Cloud Shell environment.</p>"},{"location":"GCP_DEPLOYMENT/#standard-vm","title":"Standard VM","text":"<p>This script sets up a Debian-based VM with the Langflow package, Nginx, and the necessary configurations to run the Langflow Dev environment.</p>"},{"location":"GCP_DEPLOYMENT/#spotpreemptible-instance","title":"Spot/Preemptible Instance","text":"<p>When running as a spot (preemptible) instance, the code and VM will behave the same way as in a regular instance, executing the startup script to configure the environment, install necessary dependencies, and run the Langflow application. However, due to the nature of spot instances, the VM may be terminated at any time if Google Cloud needs to reclaim the resources. This makes spot instances suitable for fault-tolerant, stateless, or interruptible workloads that can handle unexpected terminations and restarts.</p>"},{"location":"GCP_DEPLOYMENT/#pricing-approximate","title":"Pricing (approximate)","text":"<p>For a more accurate breakdown of costs, please use the GCP Pricing Calculator </p> Component Regular Cost (Hourly) Regular Cost (Monthly) Spot/Preemptible Cost (Hourly) Spot/Preemptible Cost (Monthly) Notes 100 GB Disk - $10/month - $10/month Disk cost remains the same for both regular and Spot/Preemptible VMs VM (n1-standard-4) $0.15/hr ~$108/month ~$0.04/hr ~$29/month The VM cost can be significantly reduced using a Spot/Preemptible instance Total $0.15/hr ~$118/month ~$0.04/hr ~$39/month Total costs for running the VM and disk 24/7 for an entire month"},{"location":"community/","title":"\ud83d\udc26 Stay tunned for LangFlow on Twitter","text":"<p>Follow @logspace_ai on Twitter to get the latest news about LangFlow. </p>"},{"location":"community/#star-langflow-on-github","title":"\u2b50\ufe0f Star LangFlow on GitHub","text":"<p>You can \"star\" LangFlow in GitHub.</p> <p></p> <p>By adding a star, other users will be able to find it more easily and see that it has been already useful for others.</p> <p></p> <p> </p>"},{"location":"community/#watch-the-github-repository-for-releases","title":"\ud83d\udc40 Watch the GitHub repository for releases","text":"<p>You can \"watch\" LangFlow in GitHub.</p> <p></p> <p>If you select \"Watching\" instead of \"Releases only\" you will receive notifications when someone creates a new issue or question. You can also specify that you only want to be notified about new issues, discussions, PRs, etc.</p> <p></p> <p>Then you can try and help them solve those questions.</p> <p>Thanks! \ud83d\ude80</p>"},{"location":"contribute/","title":"\ud83d\udc4b Contributing","text":"<p>Hello there! We welcome contributions from developers of all levels to our open-source project on GitHub. If you'd like to contribute, please check our contributing guidelines and help make LangFlow more accessible.</p> <p></p> <p>As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infra, or better documentation.</p> <p></p> <p>To contribute to this project, please follow a \"fork and pull request\" workflow. Please do not try to push directly to this repo unless you are a maintainer.</p>"},{"location":"conversation-buffer-memory/","title":"Conversation Buffer Memory","text":"<p><code>ConversationBufferMemory</code> allows you to store messages and then extract the messages in a variable. The memory key input is typically generated by encoding the input text using an encoder network, which maps the input text into a fixed-dimensional vector representation.</p> <p></p> <p> </p> <p></p> <p>Learn more about the ConversationBufferMemory in the LangChain documentation.</p>"},{"location":"conversation-buffer-memory/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>ConversationChain</code> is a chain to have a conversation and load context from memory. Output Key and Input Key are simply unique identifiers used to represent the data being passed between different modules or steps in a Conversation Chain. These keys help to ensure that the data is properly routed and processed by the appropriate modules in the conversation flow.</p> <p></p> <p>Output Key used the default: <code>response</code> and Input Key used the default: <code>input</code>.</p> <p></p> <p>In the LangFlow example, we used <code>ChatOpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, ChatOpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the ChatOpenAI documentation to learn more about the API and the options that contain in the node.</p>"},{"location":"conversation-summary-memory/","title":"Conversation Summary Memory","text":"<p><code>ConversationSummaryMemory</code> creates a summary of the conversation over time, which can be useful for condensing information. The memory key input is typically generated by encoding the input text using an encoder network, which maps the input text into a fixed-dimensional vector representation.</p> <p></p> <p> </p> <p></p> <p>To understand more, check out the LangChain ConversationSummaryMemory documentation.</p>"},{"location":"conversation-summary-memory/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p>In the LangFlow example, we used <code>ChatOpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, ChatOpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the ChatOpenAI documentation to learn more about the API and the options that contain in the node.</p> <p></p> <p><code>ConversationChain</code> is a chain to have a conversation and load context from memory. Output Key and Input Key are simply unique identifiers used to represent the data being passed between different modules or steps in a Conversation Chain. These keys help to ensure that the data is properly routed and processed by the appropriate modules in the conversation flow.</p> <p></p> <p>Output Key used the default: <code>response</code> and Input Key used the default: <code>input</code>.</p>"},{"location":"create-flows/","title":"Creating Flows","text":"<p>Creating flows with LangFlow is easy. Simply drag sidebar components onto the canvas and connect them together to create your pipeline. LangFlow provides a range of LangChain components to choose from, including LLMs, prompt serializers, agents, and chains.</p> <p></p> <p>Explore by editing prompt parameters, link chains and agents, track an agent's thought process, and export your flow.</p> <p></p> <p>Once you're done, you can export your flow as a JSON file to use with LangChain. To do so, click the \"Export\" button in the top right corner of the canvas, then in Python, you can load the flow with:</p> <pre><code>from langflow import load_flow_from_json\nflow = load_flow_from_json(\"path/to/flow.json\")\n# Now you can use it like any chain\nflow(\"Hey, have you heard of LangFlow?\")\n</code></pre>"},{"location":"csv-loader/","title":"CSVLoader","text":"<p>The <code>CSVLoader</code> loads a CSV file into a list of documents.</p> <p></p> <p> </p> <p></p> <p>Check out more about the <code>CSVLoader</code> in LangChain documentation.</p>"},{"location":"csv-loader/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>File path:</code></p> <p></p> <p>Download CSV</p> <p></p> <p><code>CharacterTextSplitter</code> implements splitting text based on characters.</p> <p>Text splitters operate as follows:</p> <ul> <li> <p>Split the text into small, meaningful chunks (usually sentences).</p> </li> <li> <p>Combine these small chunks into larger ones until they reach a certain size (measured by a function).</p> </li> <li> <p>Once a chunk reaches the desired size, make it its piece of text and create a new chunk with some overlap to maintain context.</p> </li> </ul> <p>Separator used:</p> <pre><code>.\n</code></pre> <p>Chunk size used:</p> <pre><code>2000\n</code></pre> <p>Chunk overlap used:</p> <pre><code>200\n</code></pre> <p> For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p> <p></p> <p>The <code>OpenAIEmbeddings</code>, wrapper around OpenAI Embeddings models. Make sure to get the API key from the LLM provider, in this case OpenAI.</p> <p></p> <p><code>Chroma</code> vector databases can be used as vector stores to conduct a semantic search or to select examples, thanks to a wrapper around them.</p> <p></p> <p>A <code>VectorStoreInfo</code> set information about the vector store, such as the name and description.</p> <p></p> <p>Name used:</p> <pre><code>organizations-100\n</code></pre> <p>Description used:</p> <pre><code>A table contains 100 companies.\n</code></pre> <p>The <code>VectoStoreAgent</code>is an agent designed to retrieve information from one or more vector stores, either with or without sources.</p>"},{"location":"github-issues/","title":"\ud83d\udea9GitHub Issues","text":"<p>Our issues page is kept up to date with bugs, improvements, and feature requests. There is a taxonomy of labels to help with sorting and discovery of issues of interest.</p> <p></p> <p>If you're looking for help with your code, consider posting a question on the GitHub Discussions board. Please understand that we won't be able to provide individual support via email. We also believe that help is much more valuable if it's shared publicly, so that more people can benefit from it.</p> <ul> <li> <p>Describing your issue: Try to provide as many details as possible. What   exactly goes wrong? How is it failing? Is there an error?   \"XY doesn't work\" usually isn't that helpful for tracking down problems. Always   remember to include the code you ran and if possible, extract only the relevant   parts and don't just dump your entire script. This will make it easier for us to   reproduce the error.</p> </li> <li> <p>Sharing long blocks of code or logs: If you need to include long code,   logs or tracebacks, you can wrap them in <code>&lt;details&gt;</code> and <code>&lt;/details&gt;</code>. This   collapses the content   so it only becomes visible on click, making the issue easier to read and follow.</p> </li> </ul>"},{"location":"github-issues/#issue-labels","title":"Issue labels","text":"<p>See this page for an overview of the system we use to tag our issues and pull requests.</p>"},{"location":"guidelines/","title":"Guidelines","text":""},{"location":"guidelines/#component","title":"Component","text":"<p>A component is a self-contained and reusable building block in software development. It is a modular unit that performs a specific function or task within a larger system. They are created to provide a convenient and straightforward way to work with language models.</p> <p> Component agents can refer to an entity that is capable of performing actions or making decisions autonomously or on behalf of someone or something else. In the case of a language model like ChatGPT, the model itself can be considered an agent as it can generate responses and interact with users based on the input it receives.</p> <p> If want to learn more about the components and how they work, make sure to check out the LangChain documentation section.</p>"},{"location":"guidelines/#compenents-features","title":"Compenent's Features","text":"<p>During the flow creation process, you will notice a colored circle o. Components marked with a red asterisk * must be connected. If you don't connect it, a red line will appear around it. Make the necessary connections to make your flow work. Hovering over the small circle will reveal the component that needs to be connected.</p> <p></p> <p>In some components, at the top of it, you will see a small gear icon \u2699\ufe0f, which you can click to edit the parameters. You also have the option to delete it by clicking the trash can icon \ud83d\uddd1\ufe0f.</p> <p></p> <p> </p>"},{"location":"guidelines/#features","title":"Features","text":"<p>Located in the right top corner of the screen there are some features that you can use, such as Code, Import, Export, Dark Mode and Notification, as you can see in the image below:</p> <p></p> <p> </p> <p></p> <p>Further down, we will explain each of these features.</p>"},{"location":"guidelines/#code","title":"Code","text":"<p>API Access: Export Your Flow for Code Usage. The API Access feature allows you to export your flow from the platform and utilize it with your own code. This feature provides two different tabs within the platform, the first being the \"Python API\" tab and the second being the \"Python Code\" tab. Each tab offers a unique set of functionalities to integrate the exported flow into your codebase seamlessly.</p> <p></p> <p>Python API Tab:</p> <p>To access the Python API tab, you can utilize the code snippet in the first tab. You can import the required libraries and define a predict function. This function takes a message as input and performs the following steps:</p> <ul> <li>Opens the \"<code>Conversation_buffer_memory.json</code>\" file, which contains the exported flow information.</li> <li>Constructs a payload consisting of the exported flow data and the input message.</li> <li>Sends a POST request to the specified API URL with the payload as JSON.</li> <li>Returns the response as a JSON object, which includes the predicted result.</li> </ul> <p></p> <p>Python Code Tab:</p> <p>To access the Python Code tab, you can utilize the code snippet in the secon tab. You can import the <code>load_flow_from_json</code> function from the \"<code>langflow</code>\" library. This function loads the exported flow from the \"<code>Conversation_buffer_memory.json</code>\" file and assigns it to the variable <code>flow</code>. Once the flow is loaded, you can use it as a chain to process input messages. In the provided example, the flow variable is used to process the message \"<code>Hey, have you heard of LangFlow?</code>\".</p> <p></p> <p>By utilizing the Python Code tab, you can seamlessly integrate the exported flow into your code and leverage its capabilities for natural language processing tasks.</p> <p></p> <p>The API Access feature empowers you to leverage the full potential of your exported flow by seamlessly integrating it into your codebase. Whether you want to incorporate advanced conversational capabilities or automate specific tasks, this feature provides a flexible and efficient solution to enhance your conversational applications.</p>"},{"location":"guidelines/#import-and-export","title":"Import and Export","text":"<p>Flows can be exported and imported as JSON files. We already have some examples on Import option, check them out.</p> <p></p> <p> </p> <p></p> <p>The Export option allows you to export your flow setting a name and description. You have the option to save the file with your API keys.</p> <p></p> Import Export"},{"location":"guidelines/#dark-mode-and-notifications","title":"Dark Mode and Notifications","text":"<p>The background color can be set to dark \ud83c\udf19 or light \u2600\ufe0f mode. The bell icon \ud83d\udd14 indicates that the component has a notification.</p>"},{"location":"guidelines/#chat","title":"Chat","text":"<p>A chat icon \ud83d\udcac located in the bottom right corner of the screen allows you to chat. When you click over \ud83d\udcac a new screen will pop up. You can start a conversation by typing in the text box and pressing enter. The chat will respond to your message. In the top right corner of the screen, you will see an eraser icon  . Clicking on it will clear the chat history.</p> <p></p> <p> </p>"},{"location":"hugging-face-spaces/","title":"Hugging Face Spaces","text":"<p>LangFlow can be used on Hugging Face Spaces. A fully featured version of LangFlow can be accessed in your browser with just one click.</p> <p> </p> <p> Check out LangFlow on Hugging Face Spaces.</p>"},{"location":"installation/","title":"How to install?","text":""},{"location":"installation/#installation","title":"Installation","text":"<p>You can install LangFlow from pip:</p> <pre><code>pip install langflow\n</code></pre> <p>Next, run:</p> <pre><code>python -m langflow\n</code></pre> <p>or</p> <pre><code>langflow\n</code></pre>"},{"location":"installation/#run-locally","title":"Run Locally","text":"<p>Run locally by cloning the repository and installing the dependencies. We recommend using a virtual environment to isolate the dependencies from your system.</p> <p></p> <p>Before you start, make sure you have the following installed:</p> <ul> <li>Poetry</li> <li>Node.js</li> </ul> <p>For the backend, you will need to install the dependencies and start the development server.</p> <pre><code>poetry install\nmake run_backend\n</code></pre> <p>For the frontend, you will need to install the dependencies and start the development server.</p> <pre><code>cd src/frontend\nnpm install\nnpm start\n</code></pre>"},{"location":"installation/#docker-compose","title":"Docker compose","text":"<p>This will run the backend and frontend in separate containers. The frontend will be available at <code>localhost:3000</code> and the backend at <code>localhost:7860</code>.</p> <pre><code>docker compose up --build\n# or\nmake dev build=1\n</code></pre>"},{"location":"json-agent/","title":"JSON Agent","text":"<p>The <code>JsonAgent</code> is an agent designed to interact with large JSON/dict objects.</p> <p></p> <p> </p> <p></p> <p>To understand more, check out the LangChain JsonAgent documentation.</p>"},{"location":"json-agent/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>JsonSpec</code> will define the Max value length of the input and output of the agent. You can get the Path file here.</p> <p></p> <p>Max value length:</p> <pre><code>400\n</code></pre> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p> <p></p> <p><code>JsonToolkit</code> for interacting with the JSON spec.</p>"},{"location":"langflow-examples/","title":"\ud83d\udd04 How to Upload Examples","text":"<p>We welcome all examples that can help our community learn and explore the features of our open-source software. Langflow Examples is a repository on GitHub that contains examples of flows that you can upload to Langflow.</p> <p>To upload examples to Langflow, please follow these steps:</p> <ol> <li> <p>Create a Flow: First, create a flow using Langflow. You can use any of the available templates or create a new flow from scratch.</p> </li> <li> <p>Export the Flow:: Once you have created a flow, export it as a .json file. Make sure to give your file a descriptive name and include a brief description of what your flow does.</p> </li> <li> <p>Submit a Pull Request: Finally, submit a pull request (PR) to the examples folder. Make sure to include your .json file in the PR.</p> </li> </ol> <p>Here are some additional guidelines to follow when submitting examples:</p> <p>Please make sure that your example follows the \u26d3\ufe0f Langflow code of conduct.</p> <p>If your example uses any third-party libraries or packages, please include them in your PR.</p>"},{"location":"langflow/","title":"\u26d3\ufe0f LangFlow","text":"<p>     ~ A User Interface For     LangChain \ud83e\udd9c\ud83d\udd17 ~ </p> <p></p> <p> </p> <p> </p> <p></p> <p>LangFlow is a GUI for LangChain, designed with react-flow. The drag-and-drop feature provides a quick and effortless way to experiment and prototype, and the built-in chat interface enables real-time interaction. With LangFlow, you can edit prompt parameters, create chains and agents, track the agent\u2019s thought process, and export your flow.</p>"},{"location":"llmchain/","title":"LLMChain","text":"<p>The <code>LLMChain</code> is a simple chain that takes in a prompt template, formats it with the user input, and returns the response from an LLM.</p> <p></p> <p> </p> <p></p> <p>More information about the LLMChain can be found in the LangChain documentation.</p>"},{"location":"llmchain/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p>The <code>PromptTemplate</code> is a simple template that takes in a product name and returns a prompt. The prompt is used to generate the response from the LLM.</p> <p></p> <p>Template:</p> <pre><code>I want you to act as a naming consultant for new companies.\n\nHere are some examples of good company names:\n\n- search engine, Google\n- social media, Facebook\n- video sharing, YouTube\n\nThe name should be short, catchy, and easy to remember.\n\nWhat is a good name for a company that makes {product}?\n</code></pre> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p>"},{"location":"llms/","title":"Llms","text":""},{"location":"llms/#openai","title":"OpenAI","text":""},{"location":"llms/#options-available","title":"Options available:","text":"<ul> <li> <p>Model name - typically refers to the name given to a specific pre-trained model or architecture used to perform a particular task. These models are usually developed by training neural networks on large datasets to learn patterns in the data, and then fine-tuning them for specific applications.</p> <ul> <li>text-davinci-003 - is a transformer-based neural network. Most capable GPT-3 model. Can do any task the other models can do, often with higher quality.</li> <li>text-davinvi-002 - is a transformer-based neural network. The 002 suffix distinguishes it from other versions of the \"davinci\" model.</li> <li>text-curie-001 - very capable, faster, and lower cost than Davinci.</li> <li>text-babbage-001 - capable of straightforward tasks, very fast, and lower cost.</li> <li>text-ada-001 - capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.</li> </ul> </li> <li> <p>Temperature - the temperature parameter controls the \"softness\" of the probability distribution produced by the softmax function. A high-temperature value produces a softer probability distribution, which means that the model will be more uncertain and assign more similar probabilities to multiple classes. A low-temperature value produces a sharper probability distribution, which means that the model will be more confident and assign higher probabilities to the most likely classes.</p> </li> <li> <p>Max tokens - refers to the maximum number of tokens (i.e., words and symbols) that can be input to the model at once for text generation or other language tasks. The exact value of the max tokens parameter may vary depending on the specific LLM variant being used and the resources available for processing the input text.</p> </li> <li> <p>Model kwargs - by adjusting the values of the kwargs, it is possible to modify the way the model is trained, how it handles inputs, or how it generates outputs. However, it is important to be careful when modifying model kwargs, as the wrong configuration can lead to poor performance or even failure of the model.</p> </li> </ul>"},{"location":"llms/#chatopenai","title":"ChatOpenAI","text":"<p>Wrapper around OpenAI Chat large language model.</p>"},{"location":"llms/#options-available_1","title":"Options available:","text":"<ul> <li> <p>Model name:</p> <ul> <li>gpt-3.5-turbo - the GPT-3.5-Turbo model has the capability unlocks some interesting features, such as the ability to store prior responses or query with a predefined set of instructions with context.</li> <li>gpt-4 - the latest milestone in OpenAI\u2019s effort in scaling up deep learning.</li> <li>gpt-4-32k - it can process as many as 32,768 tokens, which is about 50 pages of text. </li> </ul> </li> <li> <p>Max tokens - refers to the maximum number of tokens (i.e., words and symbols) that can be input to the model at once for text generation or other language tasks. The exact value of the max tokens parameter may vary depending on the specific LLM variant being used and the resources available for processing the input text.</p> </li> <li> <p>Model kwargs - by adjusting the values of the kwargs, it is possible to modify the way the model is trained, how it handles inputs, or how it generates outputs. However, it is important to be careful when modifying model kwargs, as the wrong configuration can lead to poor performance or even failure of the model.</p> </li> <li> <p>Max tokens - refers to the maximum number of tokens (i.e., words and symbols) that can be input to the model at once for text generation or other language tasks.</p> </li> </ul>"},{"location":"llms/#llama-cpp","title":"Llama Cpp","text":"<p>A wrapper around the llama.cpp model.</p> <p>Make sure you are following all instructions to install all necessary model files model.</p> <p>There is no need for API_TOKENS!</p>"},{"location":"llms/#options-available_2","title":"Options available:","text":"<ul> <li> <p>Model path: insert your model path after you have downloaded the model files.</p> </li> <li> <p>Max tokens - refers to the maximum number of tokens (i.e., words and symbols) that can be input to the model at once for text generation or other language tasks. The exact value of the max tokens parameter may vary depending on the specific LLM variant being used and the resources available for processing the input text.</p> </li> <li> <p>Temperature - the temperature parameter controls the \"softness\" of the probability distribution produced by the softmax function. A high-temperature value produces a softer probability distribution, which means that the model will be more uncertain and assign more similar probabilities to multiple classes. A low-temperature value produces a sharper probability distribution, which means that the model will be more confident and assign higher probabilities to the most likely classes.</p> </li> </ul>"},{"location":"local-development/","title":"Docker compose","text":"<p>This will run the backend and frontend in separate containers. The frontend will be available at <code>localhost:3000</code> and the backend at <code>localhost:7860</code>. <pre><code>docker compose up --build\n# or\nmake dev build=1\n</code></pre></p>"},{"location":"locally/","title":"\ud83d\udcbb Run Locally","text":"<p>Run locally by cloning the repository and installing the dependencies. We recommend using a virtual environment to isolate the dependencies from your system.</p> <p></p> <p>Before you start, make sure you have the following installed:   - Poetry   - Node.js</p> <p>For the backend, you will need to install the dependencies and start the development server. <pre><code>poetry install\nmake run_backend\n</code></pre> For the frontend, you will need to install the dependencies and start the development server. <pre><code>cd src/frontend\nnpm install\nnpm start\n</code></pre></p>"},{"location":"midjourney-prompt-chain/","title":"MidJourney Prompt Chain","text":"<p>With <code>MidJourneyPromptChain</code>, you can use it to generate new MidJourney prompts. You can just type anything you like to build an image script.</p> <p></p> <p> </p>"},{"location":"midjourney-prompt-chain/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>ConversationSummaryMemory</code> creates a summary of the conversation over time, which can be useful for condensing information. The memory key input is typically generated by encoding the input text using an encoder network, which maps the input text into a fixed-dimensional vector representation.</p> <p></p> <p>In the example, we typed the word <code>Dragon</code> and the chatbot generated a prompt for the image.</p> <p></p> <p>Check out the image created by the prompt here.</p> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p>"},{"location":"multiple-vectorstores/","title":"Multiple Vectorstores","text":"<p><code>VectorStoreRouterAgent</code> construct an agent that routes between multiple vectorstores.</p> <p></p> <p> </p> <p></p> <p>For more information about VectorStoreRouterAgent, check out the LangChain documentation.</p>"},{"location":"multiple-vectorstores/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>TextLoader</code> loads text from a file.</p> <p></p> <p>Download txt</p> <p></p> <p>By using <code>WebBaseLoader</code>, you can load all text from webpages into a document format that we can use downstream. Web path used:</p> <pre><code>https://beta.ruff.rs/docs/faq/\n</code></pre> <p><code>CharacterTextSplitter</code> implements splitting text based on characters.</p> <p>Text splitters operate as follows:</p> <ul> <li> <p>Split the text into small, meaningful chunks (usually sentences).</p> </li> <li> <p>Combine these small chunks into larger ones until they reach a certain size (measured by a function).</p> </li> <li> <p>Once a chunk reaches the desired size, make it its piece of text and create a new chunk with some overlap to maintain context.</p> </li> </ul> <p>Separator used:</p> <pre><code>.\n</code></pre> <p>Chunk size used:</p> <pre><code>2000\n</code></pre> <p>Chunk overlap used:</p> <pre><code>200\n</code></pre> <p></p> <p>The <code>OpenAIEmbeddings</code>, wrapper around OpenAI Embeddings models. Make sure to get the API key from the LLM provider, in this case OpenAI.</p> <p></p> <p><code>Chroma</code> vector databases can be used as vectorstores to conduct a semantic search or to select examples, thanks to a wrapper around them.</p> <p></p> <p><code>VectorStoreInfo</code> set information about the vectorstore, such as the name and description.</p> <p></p> <p>First VectorStoreInfo</p> <p></p> <p>Name:</p> <pre><code>state_of_union_address\n</code></pre> <p>Description:</p> <pre><code>the most recent state of the Union address\n</code></pre> <p>Second VectorStoreInfo</p> <p></p> <p>Name:</p> <pre><code>ruff\n</code></pre> <p>Description:</p> <pre><code>Information about the Ruff python linting library\n</code></pre> <p></p> <p>The <code>VectorStoreRouterToolkit</code> is a toolkit that allows you to create a <code>VectorStoreRouter</code> agent. This allows it to route between vector stores.</p> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p>"},{"location":"news-api/","title":"News API","text":"<p><code>News API</code> is a tool that allows you to get information about the top headlines of current new stories. To use the News API, you first need to sign up News API for an API key on the provider's website.</p> <p></p> <p> </p>"},{"location":"news-api/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p>"},{"location":"py-pdf-loader/","title":"PyPDFLoader","text":"<p>With <code>PyPDFLoader</code>, you can load a PDF file with pypdf and chunks at a character level.</p> <p></p> <p> </p> <p></p> <p>You can check more about the PyPDFLoader in the LangChain documentation.</p>"},{"location":"py-pdf-loader/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>File path:</code></p> <p></p> <p>Download PDF</p> <p></p> <p><code>CharacterTextSplitter</code> implements splitting text based on characters.</p> <p>Text splitters operate as follows:</p> <ul> <li> <p>Split the text into small, meaningful chunks (usually sentences).</p> </li> <li> <p>Combine these small chunks into larger ones until they reach a certain size (measured by a function).</p> </li> <li> <p>Once a chunk reaches the desired size, make it its piece of text and create a new chunk with some overlap to maintain context.</p> </li> </ul> <p>Separator used:</p> <pre><code>.\n</code></pre> <p>Chunk size used:</p> <pre><code>2000\n</code></pre> <p>Chunk overlap used:</p> <pre><code>200\n</code></pre> <p></p> <p>The <code>OpenAIEmbeddings</code>, wrapper around OpenAI Embeddings models. Make sure to get the API key from the LLM provider, in this case OpenAI.</p> <p></p> <p><code>Chroma</code> vector databases can be used as vector stores to conduct a semantic search or to select examples, thanks to a wrapper around them.</p> <p></p> <p>A <code>VectorStoreInfo</code> set information about the vector store, such as the name and description.</p> <p></p> <p>Name used:</p> <pre><code>example\n</code></pre> <p>Description used:</p> <pre><code>USENIX Example Paper.\n</code></pre> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p> <p></p> <p>The <code>VectoStoreAgent</code>is an agent designed to retrieve information from one or more vector stores, either with or without sources.</p>"},{"location":"releases/","title":"Latest Changes","text":"<p>\ud83c\udf89 We're thrilled to announce our latest release, packed with exciting updates and improvements to enhance your experience. Here's what you can look forward to:</p> <p> \ud83d\ude80 Deploy using Langchain-Serve: LangFlow can now be deployed with @JinaAI_. Thanks to @deepankarm for the contribution! \ud83d\ude4c</p> <p> \ud83d\udcd1 Copy and Paste Nodes</p> <p> \ud83d\udd19 Undo/Redo Functionality.</p> <p> And more\u2026 Bug Fixes, Validation, and Build improvements! \ud83d\udcab</p> <p> We're constantly striving to improve our platform and provide you with the best tools and features. Stay tuned for more updates as we continue to evolve and meet your needs. Thank you for being a part of our journey! \u2728\ud83d\ude80</p>"},{"location":"releases/#v0078","title":"v0.0.78","text":""},{"location":"releases/#whats-changed","title":"What's Changed","text":"<ul> <li>Hotfix: remove deepcopy, fixes llm rebuilding by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/349</li> <li>Hotfix: Deepcopy removal and bump to 0.0.77 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/350</li> <li>\ud83d\udd16 chore(pyproject.toml): bump version to 0.0.78 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/351</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.76...v0.0.78</p>"},{"location":"releases/#v0077","title":"v0.0.77","text":""},{"location":"releases/#whats-changed_1","title":"What's Changed","text":"<ul> <li>ci: test lcserve push job by @deepankarm in https://github.com/logspace-ai/langflow/pull/333</li> <li>Hotfix: Chroma does not raise NotEnoughElementsException anymore by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/334</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.75...v0.0.77</p>"},{"location":"releases/#v0076","title":"v0.0.76","text":""},{"location":"releases/#whats-changed_2","title":"What's Changed","text":"<ul> <li>ci: test lcserve push job by @deepankarm in https://github.com/logspace-ai/langflow/pull/333</li> <li>Hotfix: Chroma does not raise NotEnoughElementsException anymore by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/334</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.75...v0.0.76</p>"},{"location":"releases/#v0075","title":"v0.0.75","text":""},{"location":"releases/#whats-changed_3","title":"What's Changed","text":"<ul> <li>migrating to Vite by @gsaivinay in https://github.com/logspace-ai/langflow/pull/287</li> <li>fix: duplicated lodash by @Dogtiti in https://github.com/logspace-ai/langflow/pull/293</li> <li>Hotfix: Save Flow Bug in Browser by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/304</li> <li>Update llama-cpp-python version in pyproject.toml by @phyyou in https://github.com/logspace-ai/langflow/pull/299</li> <li>Address issue #300 - GCP Shell Script - Websocket Connection Failure by @genome21 in https://github.com/logspace-ai/langflow/pull/301</li> <li>Add health check endpoint by @filipecaixeta in https://github.com/logspace-ai/langflow/pull/311</li> <li>Hotfixes: Dark Mode classes and Text Wrapper by @lucaseduoli in https://github.com/logspace-ai/langflow/pull/309</li> <li>Validation_fix by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/317</li> <li>feat: deploy langflow using langchain-serve by @deepankarm in https://github.com/logspace-ai/langflow/pull/307</li> <li>chore: lint fix by @deepankarm in https://github.com/logspace-ai/langflow/pull/320</li> <li>Retry_get_all by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/324</li> <li>node updating template, base classes and description by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/326</li> <li>Add New Features and Improvements by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/329</li> </ul>"},{"location":"releases/#new-contributors","title":"New Contributors","text":"<ul> <li>@gsaivinay made their first contribution in https://github.com/logspace-ai/langflow/pull/287</li> <li>@Dogtiti made their first contribution in https://github.com/logspace-ai/langflow/pull/293</li> <li>@phyyou made their first contribution in https://github.com/logspace-ai/langflow/pull/299</li> <li>@filipecaixeta made their first contribution in https://github.com/logspace-ai/langflow/pull/311</li> <li>@deepankarm made their first contribution in https://github.com/logspace-ai/langflow/pull/307</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.74...v0.0.75</p>"},{"location":"releases/#v0074","title":"v0.0.74","text":""},{"location":"releases/#whats-changed_4","title":"What's Changed","text":"<ul> <li>Hotfix: saving flow in the browser local storage by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/315</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.73...v0.0.74</p>"},{"location":"releases/#v0073","title":"v0.0.73","text":""},{"location":"releases/#whats-changed_5","title":"What's Changed","text":"<ul> <li>Hotfix: Save Flow Bug in Browser by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/305</li> <li>updated pyproject version by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/306</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.72...v0.0.73</p>"},{"location":"releases/#v0072","title":"v0.0.72","text":""},{"location":"releases/#whats-changed_6","title":"What's Changed","text":"<ul> <li>Hotfix: api code in api button was missing an import by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/303</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.71...v0.0.72</p>"},{"location":"releases/#v0071","title":"v0.0.71","text":""},{"location":"releases/#whats-changed_7","title":"What's Changed","text":"<ul> <li>refactor(loading.py): change instantiate_prompt function signature to include class_object parameter and use it to instantiate the prompt object by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/302</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.70...v0.0.71</p>"},{"location":"releases/#v0070","title":"v0.0.70","text":""},{"location":"releases/#whats-changed_8","title":"What's Changed","text":"<ul> <li>Hotfix: Temporary fix for intermediate steps by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/295</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.69...v0.0.70</p>"},{"location":"releases/#v0069","title":"v0.0.69","text":""},{"location":"releases/#whats-changed_9","title":"What's Changed","text":"<ul> <li>disable password copy by @CodeAunt in https://github.com/logspace-ai/langflow/pull/224</li> <li>Fix auto update by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/238</li> <li>UI improvements by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/240</li> <li>Hot fixes by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/244</li> <li>Tool_fixes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/258</li> <li>Better_predict by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/267</li> <li>change line endings to linux. by @bobsburgers in https://github.com/logspace-ai/langflow/pull/254</li> <li>Copy Paste, Undo and Redo implemented by @lucaseduoli in https://github.com/logspace-ai/langflow/pull/253</li> <li>Streaming, Chat Markdown, and CacheManager by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/282</li> <li>Dark Mode and API Modal by @lucaseduoli in https://github.com/logspace-ai/langflow/pull/284</li> <li>fixed copy and paste bug by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/286</li> <li>Websocket, cache_manager, API access, Dark mode fixes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/285</li> </ul>"},{"location":"releases/#new-contributors_1","title":"New Contributors","text":"<ul> <li>@CodeAunt made their first contribution in https://github.com/logspace-ai/langflow/pull/224</li> <li>@bobsburgers made their first contribution in https://github.com/logspace-ai/langflow/pull/254</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.68...v0.0.69</p>"},{"location":"releases/#v0068","title":"v0.0.68","text":""},{"location":"releases/#highlights","title":"Highlights","text":"<p> Release: Chat and Cache Improvements, Websocket Integration, and Dark Mode Enhancements  This release brings a series of new features and improvements, including:</p> <ul> <li> Chat and Cache Improvements:</li> <li>Refactored cache-related functions and moved them to a new base.py module</li> <li>Simplified the Chat component and added chat history support</li> <li>Implemented the ability to send file responses in chat</li> <li>Real-time Node validation for improved user experience</li> <li>CacheManager was added to share data between tools and display them in the chat.</li> <li> Websocket Integration:</li> <li>Implemented websocket connection for the chat (WIP)</li> <li> Dark Mode Enhancements:</li> <li>Fixed dark mode for dropdown components</li> <li>Improved dark mode styling for the chat interface</li> <li>Updated thought icon for dark mode</li> <li> Other Improvements:</li> <li>Migrated chat logic to chat modal</li> <li>Implemented unique IDs for flow management</li> <li>Sorted sidebar items for better organization</li> <li>Removed unused imports and optimized codebase</li> </ul>"},{"location":"releases/#whats-changed_10","title":"What's Changed","text":"<ul> <li>refactor(loading.py): use get method to set allowed_tools to an empty list if it is not present in params dictionary by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/237</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.67...v0.0.68</p>"},{"location":"releases/#v0067","title":"v0.0.67","text":""},{"location":"releases/#whats-changed_11","title":"What's Changed","text":"<ul> <li>Fix to allowed_tools by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/235</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.66...v0.0.67</p>"},{"location":"releases/#v0066","title":"v0.0.66","text":""},{"location":"releases/#whats-changed_12","title":"What's Changed","text":"<ul> <li>Hotfix to websocket url by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/234</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.65...v0.0.66</p>"},{"location":"releases/#v0065","title":"v0.0.65","text":""},{"location":"releases/#whats-changed_13","title":"What's Changed","text":"<ul> <li>Hotfix by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/231</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.64...v0.0.65</p>"},{"location":"releases/#v0064","title":"v0.0.64","text":""},{"location":"releases/#whats-changed_14","title":"What's Changed","text":"<ul> <li>Fix LangChain imports by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/229</li> <li>fix tests to comply with updates and hotfixes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/230</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.63...v0.0.64</p>"},{"location":"releases/#v0063","title":"v0.0.63","text":""},{"location":"releases/#whats-changed_15","title":"What's Changed","text":"<ul> <li>Layout hotfixes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/223</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.62...v0.0.63</p>"},{"location":"releases/#v0062","title":"v0.0.62","text":""},{"location":"releases/#whats-changed_16","title":"What's Changed","text":"<ul> <li>Hotfix bug on import from local by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/222</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.61...v0.0.62</p>"},{"location":"releases/#v0061","title":"v0.0.61","text":""},{"location":"releases/#whats-changed_17","title":"What's Changed","text":"<ul> <li>refactor(validate.py): extract build_graph function to langflow.inter\u2026 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/204</li> <li>Update LangChain version by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/211</li> <li>Update the Semver Specifier for <code>langchain</code> to allow for all &lt;0.1.0 versions. by @darthtrevino in https://github.com/logspace-ai/langflow/pull/86</li> <li>feat: add demo devcontainer by @aaronsteers in https://github.com/logspace-ai/langflow/pull/201</li> <li>Node modal by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/174</li> <li>Auto update nodes by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/218</li> <li>Chat_and_cache by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/219</li> <li> Release: Chat and Cache Improvements, Websocket Integration, and Dark Mode Enhancements  by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/220</li> </ul>"},{"location":"releases/#new-contributors_2","title":"New Contributors","text":"<ul> <li>@darthtrevino made their first contribution in https://github.com/logspace-ai/langflow/pull/86</li> <li>@aaronsteers made their first contribution in https://github.com/logspace-ai/langflow/pull/201</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.58...v0.0.61</p>"},{"location":"releases/#v0058","title":"v0.0.58","text":""},{"location":"releases/#whats-changed_18","title":"What's Changed","text":"<ul> <li>Fix bug when running LLM alone by @ibiscp in https://github.com/logspace-ai/langflow/pull/160</li> <li>Refactor tools by @ibiscp in https://github.com/logspace-ai/langflow/pull/176</li> <li>load_flow_from_json(\"path/to/flow.json\") returns UnicodeDecodeError by @bigKeter in https://github.com/logspace-ai/langflow/pull/165</li> <li>Create a VM in a GCP project that serves the Langflow app by @genome21 in https://github.com/logspace-ai/langflow/pull/169</li> <li>Correct run instructions in Readme by @PaulLockett in https://github.com/logspace-ai/langflow/pull/87</li> <li>Add Import Examples Feature to Flow Editor by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/175</li> <li>Change button placement by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/193</li> <li>API keys for LLMs and Embeddings are now loaded from env if available by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/195</li> <li>style(langflow): fix formatting and add type hinting to custom.py and run.py files by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/194</li> <li>Example loader and bugfixes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/199</li> </ul>"},{"location":"releases/#new-contributors_3","title":"New Contributors","text":"<ul> <li>@bigKeter made their first contribution in https://github.com/logspace-ai/langflow/pull/165</li> <li>@genome21 made their first contribution in https://github.com/logspace-ai/langflow/pull/169</li> <li>@PaulLockett made their first contribution in https://github.com/logspace-ai/langflow/pull/87</li> </ul>"},{"location":"releases/#v0057","title":"v0.0.57","text":""},{"location":"releases/#whats-changed_19","title":"What's Changed","text":"<ul> <li>Hotfix: psygopg2 might break installation by @ogabrielluiz in #168</li> </ul>"},{"location":"releases/#v0056","title":"v0.0.56","text":""},{"location":"releases/#whats-changed_20","title":"What's Changed","text":"<ul> <li>Add contrib link to readme by @jacobhrussell in https://github.com/logspace-ai/langflow/pull/136</li> <li>Llama Support by @yoazmenda in https://github.com/logspace-ai/langflow/pull/134</li> <li>feat: added LLMFrontendNode by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/140</li> <li>Firefox password by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/147</li> <li>removable edges implemented by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/149</li> <li>Vector stores, embeddings, document loaders, and text splitters by @ibiscp in https://github.com/logspace-ai/langflow/pull/145</li> <li>Adding new fields to Chain nodes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/148</li> <li>Add new docloaders and change the way files are loaded by @ibiscp in https://github.com/logspace-ai/langflow/pull/154</li> <li>Change docstring parser by @ibiscp in https://github.com/logspace-ai/langflow/pull/158</li> <li>Add SQL Agent by @ibiscp in https://github.com/logspace-ai/langflow/pull/159</li> <li>VectorStores, DocumentLoaders, TextSplitters, Embeddings and other additions by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/157</li> </ul>"},{"location":"releases/#new-contributors_4","title":"New Contributors","text":"<ul> <li>@jacobhrussell made their first contribution in https://github.com/logspace-ai/langflow/pull/136</li> <li>@yoazmenda made their first contribution in https://github.com/logspace-ai/langflow/pull/134</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.55...v0.0.56</p>"},{"location":"releases/#v0055","title":"v0.0.55","text":""},{"location":"releases/#whats-changed_21","title":"What's Changed","text":"<ul> <li>feat: chain template tests by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/125</li> <li>feat: adding prompt template tests by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/128</li> <li>feat: added agents tests by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/127</li> <li>refact: more maintable build_nodes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/123</li> <li>Modifies frontend chatComponent to itemize validation errors by @cayal in https://github.com/logspace-ai/langflow/pull/126</li> <li>add minor fix for required fields that are not displayed by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/129</li> <li>feat: added first tests for llms by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/130</li> <li>Update CONTRIBUTING.md by @jordi-adame in https://github.com/logspace-ai/langflow/pull/133</li> <li>Fixes to ChainCreator and PromptTemplate processing by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/135</li> <li>Fix tool bug by @ibiscp in https://github.com/logspace-ai/langflow/pull/137</li> <li>Fix bugs, add tests, and refactor code by @ibiscp in https://github.com/logspace-ai/langflow/pull/138</li> </ul>"},{"location":"releases/#new-contributors_5","title":"New Contributors","text":"<ul> <li>@cayal made their first contribution in https://github.com/logspace-ai/langflow/pull/126</li> <li>@jordi-adame made their first contribution in https://github.com/logspace-ai/langflow/pull/133</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.54...v0.0.55</p>"},{"location":"releases/#v0054","title":"v0.0.54","text":""},{"location":"releases/#whats-changed_22","title":"What's Changed","text":"<ul> <li>Remove tools until fixed by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/124</li> <li>Fix prompt validation exception condition by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/122</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.53...v0.0.54</p>"},{"location":"releases/#v0053","title":"v0.0.53","text":""},{"location":"releases/#whats-changed_23","title":"What's Changed","text":"<ul> <li>Implementation of Agents as Tools and Custom Tools by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/79</li> <li>removed handle from code component by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/83</li> <li>refac: Factory implementation of LangChainTypes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/84</li> <li>Toolkits by @ibiscp in https://github.com/logspace-ai/langflow/pull/92</li> <li>Added initialize_agent and Memory by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/93</li> <li>fix: deactivate intermediate steps for now by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/97</li> <li>Chain loader by @ibiscp in https://github.com/logspace-ai/langflow/pull/98</li> <li>UI fixes by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/99</li> <li>colorized think with ANSI to HTML by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/101</li> <li>bug fixed when not implemented nodes are imported to the flow by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/102</li> <li>delete the message that was sent when the backend returned an error by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/105</li> <li>fixed validation error by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/106</li> <li>Implement Memories, validation and other fixes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/107</li> <li>fix: adding memory node and better exceptions by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/110</li> <li>UI improvements by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/112</li> <li>Bug fixes by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/115</li> <li>Ui improvement by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/114</li> <li>Prompt component by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/116</li> <li>added colors to toolkits and wrappers by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/118</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.46...v0.0.53</p>"},{"location":"releases/#v0052","title":"v0.0.52","text":""},{"location":"releases/#whats-changed_24","title":"What's Changed","text":"<ul> <li>fix: adding maintainers and other configs by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/59</li> <li>feat: added constants file to support model types and others by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/60</li> <li>UI updates by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/63</li> <li>full dropdown clickable by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/69</li> <li>Add config file with option to enable all features by @ibiscp in https://github.com/logspace-ai/langflow/pull/70</li> <li>UI updates by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/71</li> <li>Release 0.0.46 by @ibiscp in https://github.com/logspace-ai/langflow/pull/77</li> <li>Release 0.0.46 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/78</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.45...v0.0.52</p>"},{"location":"releases/#v0046","title":"v0.0.46","text":""},{"location":"releases/#whats-changed_25","title":"What's Changed","text":"<ul> <li>fix: adding maintainers and other configs by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/59</li> <li>feat: added constants file to support model types and others by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/60</li> <li>UI updates by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/63</li> <li>full dropdown clickable by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/69</li> <li>Add config file with option to enable all features by @ibiscp in https://github.com/logspace-ai/langflow/pull/70</li> <li>UI updates by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/71</li> <li>Release 0.0.46 by @ibiscp in https://github.com/logspace-ai/langflow/pull/77</li> <li>Release 0.0.46 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/78</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.45...v0.0.46</p>"},{"location":"releases/#v0045","title":"v0.0.45","text":""},{"location":"releases/#whats-changed_26","title":"What's Changed","text":"<ul> <li>fix: deactivate replace_port by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/56</li> <li>Release 0.0.45 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/57</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.44...v0.0.45</p>"},{"location":"releases/#v0044","title":"v0.0.44","text":""},{"location":"releases/#whats-changed_27","title":"What's Changed","text":"<ul> <li>feat: added star history to README by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/44</li> <li>fix: new frontend dev.dockerfile by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/48</li> <li>fix: new frontend dev.dockerfile by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/52</li> <li>Fix: Dynamic tooltipTitle, id mapping bugfix for last ParameterCompon\u2026 by @ScripterSugar in https://github.com/logspace-ai/langflow/pull/51</li> <li>Add gpt4 by @ibiscp in https://github.com/logspace-ai/langflow/pull/55</li> <li>Release by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/49</li> </ul>"},{"location":"releases/#new-contributors_6","title":"New Contributors","text":"<ul> <li>@ScripterSugar made their first contribution in https://github.com/logspace-ai/langflow/pull/51</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.40...v0.0.44</p>"},{"location":"releases/#v0040","title":"v0.0.40","text":""},{"location":"releases/#whats-changed_28","title":"What's Changed","text":"<ul> <li>Replace langchain by @ibiscp in https://github.com/logspace-ai/langflow/pull/31</li> <li>Windows now uses Uvicorn by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/33</li> <li>Fix docker examples by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/34</li> <li>refac: langflow_backend -&gt; langflow by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/35</li> <li>refac: remove frontend folder from backend by @ibiscp in https://github.com/logspace-ai/langflow/pull/36</li> <li>version with dinamic port by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/37</li> <li>Release 0.0.40 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/43</li> </ul>"},{"location":"releases/#new-contributors_7","title":"New Contributors","text":"<ul> <li>@ibiscp made their first contribution in https://github.com/logspace-ai/langflow/pull/31</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.33...v0.0.40</p>"},{"location":"releases/#v0033","title":"v0.0.33","text":""},{"location":"releases/#whats-changed_29","title":"What's Changed","text":"<ul> <li>Release 0.0.33 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/24</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.32...v0.0.33</p>"},{"location":"releases/#v0032","title":"v0.0.32","text":""},{"location":"releases/#whats-changed_30","title":"What's Changed","text":"<ul> <li>Release 0.0.32 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/22</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/compare/v0.0.31...v0.0.32</p>"},{"location":"releases/#v0031","title":"v0.0.31","text":""},{"location":"releases/#whats-changed_31","title":"What's Changed","text":"<ul> <li>MemoryCustom node added by @anovazzi1 in https://github.com/logspace-ai/langflow/pull/1</li> <li>Create CODE_OF_CONDUCT.md by @CarlosRodrigoCoelho in https://github.com/logspace-ai/langflow/pull/7</li> <li>feat: adding release and lint github actions by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/9</li> <li>Use uvicorn on macOS to avoid making the user set env variable by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/10</li> <li>Release 0.0.28 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/11</li> <li>Update release.yml by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/12</li> <li>Release 0.0.29 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/13</li> <li>Release 0.0.30 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/14</li> <li>Release 0.0.30 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/15</li> <li>Release 0.0.31 by @ogabrielluiz in https://github.com/logspace-ai/langflow/pull/16</li> </ul>"},{"location":"releases/#new-contributors_8","title":"New Contributors","text":"<ul> <li>@anovazzi1 made their first contribution in https://github.com/logspace-ai/langflow/pull/1</li> <li>@CarlosRodrigoCoelho made their first contribution in https://github.com/logspace-ai/langflow/pull/7</li> <li>@ogabrielluiz made their first contribution in https://github.com/logspace-ai/langflow/pull/9</li> </ul> <p>Full Changelog: https://github.com/logspace-ai/langflow/commits/v0.0.31</p>"},{"location":"series-character-chain/","title":"Series Character Chain","text":"<p>With <code>SeriesCharacterChain</code>, you can chat with the characters in the series you like most. You can just type the name of the character and the series, and the bot will start chatting with the character.</p> <p></p> <p>Character:</p> <pre><code>Gandalf\n</code></pre> <p>Series:</p> <pre><code>The Lord of the Rings\n</code></pre> <p></p> <p> </p> <p></p> <p>Play around with it and see how it works!</p>"},{"location":"series-character-chain/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p>"},{"location":"serp-api/","title":"Serp API","text":"<p><code>Search</code> is a search engine useful to answer questions about current events. To use the Serp API, you first need to sign up Serp API for an API key on the provider's website.</p> <p></p> <p>The Serp API (Search Engine Results Page API) is an API (Application Programming Interface) that allows developers to scrape search engine results from various search engines such as Google, Bing, Yahoo, and more.</p> <p></p> <p> </p> <p></p> <p>To understand more, check out the LangChain Search documentation.</p>"},{"location":"serp-api/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>ZeroShotPrompt</code> creates a prompt template for Zero-Shot Agent. You can set the Prefix and Suffix. The Prefix is the text that will be added before the input text. The Suffix is the text that will be added after the input text. In the example, we used the default that is automatically set.</p> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p> <p></p> <p>The <code>LLMChain</code> is a simple chain that takes in a prompt template, formats it with the user input, and returns the response from an LLM.</p> <p></p> <p><code>ZeroShotAgent</code> is an agent Agent for the MRKL chain. It uses a Zero Shot LLM to generate a response.</p>"},{"location":"time-travel-guide-chain/","title":"Time Travel Guide Chain","text":"<p><code>TimeTravelGuideChain</code> will provide you with the historical period or future time you want to visit and you will suggest the best events, sights, or people to experience. Output Key and Input Key are simply unique identifiers used to represent the data being passed between different modules.</p> <p></p> <p> </p>"},{"location":"time-travel-guide-chain/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p>The <code>ConversationSummaryMemory</code>. A memory of this type creates a summary of the conversation over time, which can be useful for condensing information. The memory key input is typically generated by encoding the input text using an encoder network, which maps the input text into a fixed-dimensional vector representation. We used history as default.</p> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p>"},{"location":"tool-pal-math/","title":"PAL-MATH","text":"<p><code>PAL-MATH</code> is a language model that is good at solving complex math problems. The input should be a fully worded hard-word math problem.</p> <p></p> <p> </p> <p>To understand more, check out the LangChain PAL-MATH documentation.</p>"},{"location":"tool-pal-math/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p><code>ZeroShotPrompt</code> creates a prompt template for Zero-Shot Agent. You can set the Prefix and Suffix. The Prefix is the text that will be added before the input text. The Suffix is the text that will be added after the input text. In the example, we used the default that is automatically set.</p> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p> <p></p> <p>The <code>LLMChain</code> is a simple chain that takes in a prompt template, formats it with the user input, and returns the response from an LLM.</p> <p></p> <p><code>ZeroShotAgent</code> is an agent Agent for the MRKL chain. It uses a Zero Shot LLM to generate a response.</p>"},{"location":"vectorstore-agent/","title":"Vector Store Agent","text":"<p>The <code>VectoStoreAgent</code>is an agent designed to retrieve information from one or more vectorstores, either with or without sources.</p> <p></p> <p> </p> <p></p> <p>Check out the VectoStoreAgent in the LangChain documentation.</p>"},{"location":"vectorstore-agent/#langflow-example","title":"\u26d3\ufe0fLangFlow example","text":"<p>Download Flow</p> <p></p> <p>By using <code>WebBaseLoader</code>, you can load all text from webpages into a document format that we can use downstream. Web path used:</p> <pre><code>https://beta.ruff.rs/docs/faq/\n</code></pre> <p></p> <p><code>CharacterTextSplitter</code> implements splitting text based on characters.</p> <p>Text splitters operate as follows:</p> <ul> <li> <p>Split the text into small, meaningful chunks (usually sentences).</p> </li> <li> <p>Combine these small chunks into larger ones until they reach a certain size (measured by a function).</p> </li> <li> <p>Once a chunk reaches the desired size, make it its piece of text and create a new chunk with some overlap to maintain context.</p> </li> </ul> <p>Separator used:</p> <pre><code>.\n</code></pre> <p>Chunk size used:</p> <pre><code>2000\n</code></pre> <p>Chunk overlap used:</p> <pre><code>200\n</code></pre> <p></p> <p>The <code>OpenAIEmbeddings</code>, wrapper around OpenAI Embeddings models. Make sure to get the API key from the LLM provider, in this case OpenAI.</p> <p></p> <p><code>Chroma</code> vector databases can be used as vectorstores to conduct a semantic search or to select examples, thanks to a wrapper around them.</p> <p></p> <p>A <code>VectorStoreInfo</code> set information about the vectorstore, such as the name and description.</p> <p>Name used:</p> <pre><code>ruff\n</code></pre> <p>Description used:</p> <pre><code>Information about the Ruff python linting library\n</code></pre> <p></p> <p>For the example, we used <code>OpenAI</code> as the LLM, but you can use any LLM that has an API. Make sure to get the API key from the LLM provider. For example, OpenAI requires you to create an account to get your API key.</p> <p></p> <p>Check out the OpenAI documentation to learn more about the API and the options that contain in the node.</p>"}]}